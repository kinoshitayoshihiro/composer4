以下は、「composer4」リポジトリと最新の研究動向を踏まえ、Stage 3（楽器ジェネレーター学習）の実装に向けた具体的なアイディアや提案を整理したものです。Stage 1.2までの成果を受けた次のステップとして、ラベル設計、データ統合、モデル設計、評価まで広範にまとめました。

---

## 1. 現状整理と課題

* **Stage1/Stage2（=Stage1.2）**では、76,165件のドラムループから2,064件のクリーンなループを抽出し、テンポ・拍子・プログラム・リズム指紋・マイクロタイミングなどを計測した正規化データセット（Parquet＋CSV＋JSONL）を構築済みです。Stage2では特に、**Swing比やゴーストノート率、ドラム衝突率、役割分散、Microtiming RMS**といった新しい指標を追加し、**Timing／Velocity／Groove Harmony／Drum Cohesion／Structure**の5軸で0–100点のスコアを付与する仕組みが定義されています。
* 今後は、こうした定量化されたループメタデータと既存の `tags.yaml`・`mood.yaml`、外部の楽曲ステム（Suno生成物）等を統合し、**楽器別生成モデル**を訓練するStage 3へ進む必要があります。

---

## 2. ラベル設計のアイディア

### 2.1 感情（Emotion）

* **XMIDIデータセット**は108,023曲のMIDIファイルに精緻な感情ラベル（happy, exciting, romantic, sad, warm, fear など）とジャンルラベルを付与し、感情制御付き生成モデルの訓練に使用されています。この分類を参考に、`mood.yaml` にある既存感情タグを、**5〜8種の離散ラベル**（例：happy／sad／romantic／intense／calm／fear）に整理します。
* 感情ラベルは、**valence–arousalの連続値**と併記すると柔軟性が高まります。例えば `emotion_valence ∈ [–1,1]`、`emotion_arousal ∈ [0,1]` を採用し、タグと数値の相互マッピング表を `tags.yaml` に定義します。
* Stage2のメトリクス（Ghost率やSwing比など）を特徴量として**感情予測器**を事前学習し、既存MIDIの感情推定に利用することも可能です。XMusicではマルチタスク学習により質評価・感情認識・ジャンル認識を同時に行うことで高品質生成を実現しており、同様の発想が応用できます。

### 2.2 ジャンル（Genre）

* Stage1 summaryによれば、ドラムループには funk, rock, latin, hiphop など多数のサブジャンルが含まれています。Stage3ではこれを二階層に整理しましょう。

  1. **上位ジャンル**: rock／pop／jazz／funk／latin／hiphop／electronic／classical 等。
  2. **細分類**: 既存の細かいサブジャンル（latin-brazilian-bossa 等）を上位ジャンルに紐付けます。
* ラベルはワンホットベクトルまたは整数で表現し、**複数ジャンルの重み付け**ができるようにしておきます。ジャンル未設定のデータについては、メロディ構造やコード進行から機械学習で推定する補助モデルを用意します。

### 2.3 リズム／キー（Rhythm & Key）

* 時間情報はStage2の出力に含まれる `tempo.initial_bpm`、`tempo.events`、`time_signature`（4/4, 3/4 等）に基づきます。さらに `swing_phase` (even／odd／triplet) や `microtiming_rms` などの指標をラベル化し、**on-beat／off-beat／swing感**を制御可能なパラメータとして扱います。
* 調性（Key）はLAMDaのSIGNATURES_DATAやCHORDS_DATAから自動推定できます。例えば、ピッチ分布の最頻値を使ってキー候補を導出し、メジャー／マイナーの区別も加えることで `key=C_major` のようなタグを付与します。
* 推奨するラベルスキーマ：`key_root ∈ {C, C#, D, …}`、`key_mode ∈ {major, minor, dorian, mixolydian, …}`、`meter ∈ {4/4, 3/4, 6/8, …}`、`tempo_bucket ∈ {slow, medium, fast}` 等。

### 2.4 奏法／演奏技法（Playing Technique）

* 楽器ごとの奏法ラベルは、既存ドラムループでは **kick/snare/hihatパターン**や **ghost note**、**swingの有無**などをヒントに自動抽出できます。Stage2で計算する `drum_collision_rate` や `role_separation` は役割推定の精度を高める指標です。例えば、kick・snare・hihatが同時に鳴る比率が高ければ「busy style」、ghost note比率が高ければ「funky groove」と分類できます。
* 弦楽器や鍵盤楽器については、近年提案された**演奏技法付き転写モデル VioPTT**に注目します。VioPTTは音声からバイオリンの奏法（pizzicato、spiccato、フラジョレット等）をピッチやオンセットと同時に転写するモデルであり、合成データセットMOSA-VPTを公開しています。この研究が示すように、**奏法を含むデータセットを合成で生成し、モデルに楽器特有の技法をラベルとして学習させる**方法が実践的です。
* Stage3では、ギターやベース用にも同様の奏法分類器を準備し、ピックストローク（down/up）、スラップ、ミュート、レガート、スタッカートなどをタグ付けします。奏法抽出には、音源ステム（Suno）からのオーディオ分離＋既存AMTモデル＋奏法分類モデルを組み合わせ、サンプルごとに自動ラベル付与するパイプラインを構築します。

---

## 3. データ統合と特徴量設計

1. **統合マスターデータセットの構築**

   * Stage2から得た `loop_summary.csv` と `canonical_events.parquet` をベースに、`tags.yaml`／`mood.yaml`のラベル、LAMDaのMETA_DATA（作曲家・BPM・拍子など）、Sunoステムから抽出した楽器ごとのMIDIを結合します。
   * 各楽曲（または各ループ）に対し、**基本情報（曲名・作曲者・テンポ）＋カテゴリラベル（感情・ジャンル・奏法・リズム／キー）＋数値特徴量（swing_ratio, microtiming_rms 等）**を持つJSONL/CSVを生成します。
   * ラベル欠損がある場合は、MetaScoreが採用したように機械学習で推定したり、ChatGPT等の大規模言語モデルを使ってテキストタグを生成すると良いでしょう。MetaScoreでは963K曲に対し、タグ補完および自然言語キャプションを生成することで複数の条件（楽器、ジャンル、作曲者、複雑性）をコントロールできるモデルを訓練しています。

2. **特徴量のエンコーディング**

   * 個々のノートは `onset_ticks/duration_ticks/pitch/velocity/channel/program` などで表現し、ループ全体のリズムヒストグラムやテンポ変化は `metrics.rhythm_fingerprint` や `tempo.events` を使用します。
   * ラベル情報はエンベディング層に変換し、モデルの入力として付加します。例えば `[CLS] <emotion=happy> <genre=rock> <tempo=120> <key=C_major>` のように特殊トークンとして挿入する方式が考えられます。XMusicではプロンプトパーサ「XProjector」が感情・ジャンル・リズムをシンボリック要素に変換し、Generatorに渡しています。

---

## 4. モデル・ルールエンジン設計の提案

### 4.1 モデル選択

* **楽器別生成モデル**: 各楽器（ドラム／ベース／ギター／ピアノ／ストリングスなど）ごとに専用のシーケンスモデルを用意します。Transformer系（Music Transformer、Llama-based拡張等）や拡散モデルをベースに、ラベルエンベディングを条件入力として導入します。
* **多タスク学習**: XMusicのように生成・感情認識・ジャンル認識・品質判定を同時に学習するアプローチを取り入れ、モデルに「どの感情やジャンルに近いか」を自己判定させると制御精度が上がります。
* **LoRA微調整**: 既存の大規模音楽モデル（MusicLM, MusicGen, GPT‑5ベースのMIDI生成モデル等）が利用可能であれば、LoRAで楽器別モデルを軽量に微調整できます。既存の `train_piano_lora.py` などのスクリプトや `train_velocity.py` と組み合わせると効率的です。
* **ルールベース補完**: 人間らしいニュアンスや音楽理論上の制約を反映させるため、Rule engine (quantization, swing, humanization, chord substitution) を後段に挟み込みます。リポジトリにはhumanizerやgroove_profileが存在しており、これを生成結果の後処理として使うことでクオリティを向上できます。

### 4.2 学習パイプライン

1. **データローダ**: 統合データセットから楽器ごとにミニバッチを構築し、ラベルベクターとイベントシーケンスを取得します。
2. **トークン化**: TMIDIX系のイベント表現（onset/dur/pitch/vel/program/ch）をそのままトークン化するか、RemiやCompound Word Tokenなどの形式を使います。
3. **損失関数**: 生成タスクのクロスエントロピーに加え、ラベル一致用の分類損失（感情／ジャンル予測）、Stage2の品質スコアに基づくレギュラライゼーションを加え、タイミング・ベロシティの多様性をペナルティに含めます。
4. **評価**: Stage2で定義した5軸スコアを生成物に計算し、全体スコア（p50/p90）やメトリクスの向上度を定点観測します。さらに、感情・ジャンル分類器での精度を確認し、必要であればハイパーパラメータを調整します。
5. **自動ペア作成**: Stage2の `lamda_make_pairs.py` を拡張し、基準ループと条件を元に「ポジティブ／ネガティブ」な教師データペアを自動生成します。Swing位相やリズム指紋の近さを用いて近傍ポジティブを選び、遠いものをネガティブとする方法が既に実装されています。

---

## 5. 学習・評価パイプラインの実装ステップ

1. **Stage2の再確認**: すでにdedupeされたドラムループに対して `lamda_stage2_extractor.py` を本番データに実行し、`metrics_score.jsonl` と `loop_summary.csv` を生成します。
2. **ラベル統合ツールの作成**: `lamda_dataset_builder.py` を拡張し、Stage2出力・`tags.yaml`・`mood.yaml`・LAMDa META_DATA を読み込んで統合データをJSONLに変換します。
3. **奏法ラベル付与**: MOSA‑VPTのような合成データや既存AMTモデルを活用し、各楽器の奏法を推定してタグ付けします。
4. **ラベルエンコーダ・デコーダを実装**: 感情・ジャンル・テンポ・キー・奏法を整数/ベクトルに変換するモジュール（encoder）と、逆変換するdecoderを用意します。
5. **モデル設計と試作**: まずは単一楽器（例：ピアノ）で実験し、ラベル条件が生成に与える影響を評価します。上記の評価メトリクスを用いてスコア70超を目指します。
6. **マルチタスク学習の導入**: 生成モデルに分類ヘッドを加え、自己評価能力を持たせることを検討します。
7. **人間評価**: 最終的な音楽性は人の評価も重要です。感情一致度やジャンルらしさを確認するユーザスタディを実施し、モデルの改善に反映させます。

---

## 6. 外部データの活用可能性

* **XMIDI**: 感情・ジャンルラベル付きの108k曲MIDIデータセットで、感情制御付き生成の訓練に適します。
* **VioPTT／MOSA‑VPT**: 奏法ラベル付きのバイオリン合成データセットを用い、奏法分類器を自前で訓練するヒントを提供します。
* **MetaScore**: 96万曲の楽譜とジャンル・作曲者・キー・拍子など豊富なメタデータが含まれ、欠損ジャンルを推定するアルゴリズムやLLMによるテキスト生成の手法が参考になります。
* これら外部データセットをLAMDaの連邦制データモデルに取り込み、タグの補完・モデルの事前学習・評価ベンチマークに使用すると効果的です。

---

## 7. 実行プランとタイムライン（概案）

| 期間         | 主要タスク                                                                                                                 |
| ---------- | --------------------------------------------------------------------------------------------------------------------- |
| **10月第2週** | Stage2本番実行（全データへの `lamda_stage2_extractor.py` 実行）。`loop_summary.csv`とスコア分布を確認し、閾値や重みを微調整する。                           |
| **10月第3週** | 統合データセット生成ツールを開発し、`tags.yaml`／`mood.yaml`／LAMDa META_DATA を結合したマスターデータ `master_dataset.jsonl` を作成。奏法分類器の下調べとラベル設計に着手。 |
| **10月第4週** | Piano/Bass など単一楽器で条件付きTransformerを試作。ラベルエンコーディングを実装し、短いループの生成実験を実施。Stage2指標で品質を検証。                                    |
| **11月前半**  | 奏法分類器を実装し、MOSAVPTやSunoステムから奏法ラベルを推定。マルチタスクモデル（生成＋分類）への拡張を検討。                                                          |
| **11月後半**  | 複数楽器（ドラム・ベース・ギター）のモデルを並行して訓練。XMusicやMetaScoreの手法を参考に、質評価・感情認識・ジャンル認識を共同学習させる実験を開始。                                    |
| **12月以降**  | Streamlit/CLIによる生成ツール化、ルールエンジンとの統合、ユーザ評価の実施。外部データセットの取り込みと追加学習を進め、自己増殖型サイクルに組み込む。                                     |

---

## 8. おわりに

Stage 3は、単なる学習フェーズではなく、**LAMDaが描く「音楽の知識 → 類似検索 → 生成応用」という循環の核心**です。感情・ジャンル・リズム／キー・奏法の各ラベルを精緻に設計し、多タスク学習や外部データも積極的に活用することで、ユーザーの意図に合った表現力豊かな楽曲生成が実現できるでしょう。

この計画が、Harusanのクリエイティブな音楽世界をさらに広げる一助となれば幸いです。
