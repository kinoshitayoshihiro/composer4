ディープラーニングの利用
各データセット（XMIDI, VioPTT, MOSA‑VPT, MetaScore）について詳細に調査

Q
ライセンスや再配布の可否、リポジトリのシステムに導入可能かどうか？　利用条件から見合わなければみおくります。

stage3　dataのラベリングに使いたいのですが、そのためには
stage2　のMIDIからのdata抽出も見直さなければならないと考えています。

現在の主目的は、stage3の、感情・ジャンル・リズムキー・奏法、といったラベルを活用、モデル・ルールエンジンの構築です。
楽器別フレーズ生成につなげます。

・人間の生演奏に限りなく近づける、
・DAWにおける、プラグインの性能を限りなく引き出す
・わたしの製作しているのが、自分で製作したAudioBook主題歌。楽曲としては、「物語cipher」を意識しています。cipherといっても、型にとらわれない自由な物です。物語とは、小説の主題化なので、それにあわせて作詞も物語形式にしています。

「各projectの目的と成果」
　これも各データセットの性格と機能を捉えるのに重要です。

・わたしの製作しているのが、自分で製作したAudioBook主題歌。楽曲としては、「物語cipher」を意識しています。cipherといっても、型にとらわれない自由な物です。物語とは、小説の主題化なので、それにあわせて作詞も物語形式にしています。

ⅡDeepリサーチ結果。


XMIDIデータセットとは

XMIDIは、Tencentの研究チームが提案した大規模シンボリック音楽データセットであり、感情（Emotion）とジャンル（Genre）のラベルを各楽曲に付与している点が特徴です
xmusic-project.github.io
。開発時期は2023年前後で、2025年に学術論文として発表されました（WAIC 2023にてプロジェクトが注目を集めています
arxiv.org
）。このデータセットは既存の公開楽曲データ集が規模・ラベル情報の面で不十分であったことを踏まえ、そのギャップを埋める目的で構築されました
xmusic-project.github.io
。具体的には108,023曲ものMIDIファイル（平均演奏時間約176秒）を収録し、総データ長は約5,278時間にも及びます
github.com
。各ファイル名に感情とジャンルのラベルが含まれており、精緻な分類がなされています。このXMIDIデータセットは、マルチモーダルなプロンプトから感情制御可能な音楽生成を行うフレームワーク「XMusic」の中核データとして利用されており、同手法は従来の音楽生成モデルを質・量ともに大きく上回る成果を示しました
xmusic-project.github.io
arxiv.org
。要するに、XMIDIは「楽曲の感情表現とジャンル」を明示した世界最大規模のMIDIデータセットとして、感情やジャンルをコントロール可能な音楽AIの研究を加速するために作成されたものです。

VioPTTとMOSA‑VPT

VioPTT（Violin Playing Technique-aware Transcription）は2025年に提案された最新の音楽情報処理手法で、バイオリン音源から演奏テクニックまで含めて自動採譜（トランスクリプション）するモデルです
arxiv.org
。従来の自動楽譜起こしモデルは音高やタイミングの検出に留まり、ビブラートやボウイングといった楽器固有の表現ニュアンスは省略されがちでした。VioPTTはこの課題に対し、音高・発音タイミングに加えて各音符に対応するバイオリン奏法（演奏テクニック）を同時に推定する世界初の試みを行っています
arxiv.org
。その目的は、人間の演奏表現に近い豊かな情報を得ることで、より表情豊かな音楽解析・生成を可能にすることです。

このモデルの開発にあたって鍵となったのが、付随して公開されたMOSA‑VPTデータセットです。MOSA‑VPTは「Music mOtion with Semantic Annotation – Violin Playing Technique」の略で、バイオリンの演奏奏法に特化した高品質な合成データセットとして新たに構築されました
arxiv.org
。MOSAという既存の音楽モーションキャプチャデータセットの知見を活用しつつ、専門家による手作業のアノテーションに頼らず合成データで奏法ラベル付き音源を大量生成することを目指しています
arxiv.org
。具体的には、4種類のバイオリン奏法（デタシェ、フラジオレット、スピッカート、ピチカート）それぞれについてバランス良く音源が含まれるよう設計されており、総計76時間分の音声-MIDIペアを収録しています
arxiv.org
。各音符にどの奏法が使われているかが明確に対応付けられており、大規模ながら正確な奏法ラベルデータを実現しています。

MOSA-VPTの特徴は、完全にソフト音源で合成されたデータであることです。具体的には、Python製の音響プラグインホスト環境「DAWDreamer」を用いて、Vienna社のSynchron Solo Violin Iというプロ仕様のバーチャル楽器音源をホストし
arxiv.org
、MIDIデータに対してプログラム的にキー・スイッチやCC（連続コントローラ）信号を操作することで指定した奏法で演奏させるという手法を採りました
arxiv.org
。例えばある音符列をデタシェ奏法で演奏する音源、同じ音符列をピチカートで演奏する音源、というように自在に生成できます。残響や定位の効果は切り、モノラル16kHzでレンダリングすることで、生録音との差異（ドメインギャップ）を抑えています
arxiv.org
。このように完全自動・大規模に生成されたデータであるため、コストのかかる人手によるラベリングを回避でき、しかもバイオリン以外の楽器でも対応可能な柔軟性を持つ点が画期的です
arxiv.org
。

VioPTTモデルの成果として、合成データで訓練したにも関わらず実音源に対する優れた汎化性能を示し、既存の最先端モデルを上回る精度でバイオリン音源の音高・奏法を同時推定できたことが報告されています
arxiv.org
。MOSA-VPTデータセット自体も、論文採択後に研究目的で公開予定である旨が示されており
arxiv.org
、今後バイオリン奏法認識や表現コントロールの研究資源として活用される見込みです。

MetaScoreデータセット

MetaScoreとは、2025年の国際会議 ISMIR にて発表予定の大規模楽譜データセットおよびそれを用いたシンボリック音楽生成手法です
github.com
。米デューク大学などの研究者によって構築されました。その最大の特徴は、オンライン楽譜共有プラットフォーム「MuseScore」のフォーラムから収集した膨大なユーザ投稿楽譜データ（約96.3万曲）に、豊富なメタデータを付与している点です
github.com
。各楽曲にはユーザが入力したジャンルタグや楽器編成、作曲者、難易度等の情報が含まれ、さらに研究チームはそのメタ情報をもとに大規模言語モデル(LLM)を用いて楽曲の内容説明文（キャプション）を自動生成しています
github.com
。こうしたデータ拡張により、単に音符の集合としての楽譜だけでなく、「どんな雰囲気・スタイルの曲か」「どういったコンセプトで書かれたか」といったセマンティックな情報まで含めた新種のデータセットとなっています
github.com
。

MetaScoreデータセットは構成上いくつかのサブセットに分けられます
github.com
：

MetaScore-Raw (生データ): MuseScoreフォーラムからスクレイピングした963,000曲分のMuseScoreファイルとメタデータ、およびそれをMusicXML形式に変換したもの。生のユーザ投稿情報をそのまま含みます
github.com
。

MetaScore-Genre (ジャンルサブセット): 上記からユーザがジャンルを明示してタグ付けしている約181,000曲を抽出したもの。加えて、作曲者ごとの曲数が極端に少ないものは除外し（100曲未満の作曲者を除くことで、有名作曲者や多作な投稿者に絞る）、各楽曲に対してLLM生成の曲説明文を付与しています
github.com
。

MetaScore-Plus (補完データ): Rawの96万曲全てについて、ジャンルタグが無い曲には学習済みジャンル分類器でジャンル推定を補完し、さらに全曲にLLM生成キャプションを付けたもの。いわば最大版のデータセットです
github.com
。

本プロジェクトの目的は、この巨大な楽譜＋メタ情報コーパスを用いて**「テキストから音楽へ」(text-to-music)の生成AIを構築することにあります。実際、提案されたMetaScore Transformer (MST)というモデルは、ユーザが入力する自然言語の文章＋（任意で指定する難易度・ジャンル・使用楽器・作曲家といった条件）から、それに見合ったシンボリック音楽（MIDI形式の楽譜データ）を生成します
github.com
。これは、単にタグ条件付きで曲を生成する従来手法より一歩進んで、物語風の説明文や音楽的コンセプト**から実際の楽曲を生み出せる点で非常に挑戦的な成果です。研究チームはデモページ上で、与えた文章 prompt に応じてMSTモデルが出力した楽譜例を公開しており、自由なテキストによる音楽コントロールの可能性を示しています。

なお、データ容量と著作権に関する注意点として、MetaScoreは他者の作曲作品も含むユーザ投稿データを由来とするため、その大部分が著作権保護下にあります。研究チームは公的に再配布可能な部分のみをオープンデータとして公開予定であり、具体的にはパブリックドメイン（著作権保護期間切れ）楽曲約228,000曲と、クリエイティブ・コモンズライセンスで提供されている楽曲約46,000曲を抜き出し、それらについて楽譜とメタデータを誰でも利用できる形でリリースすると述べています
github.com
。これ以外の約69万曲分については、研究目的に限り個別リクエストに応じて提供する方針です
github.com
。要するに、全体の約3割（274k曲）は自由利用可だが残り7割強は公開データとしては扱えないという制約があります。この点は後述するリポジトリ統合時の検討事項で重要になります。

ライセンスおよびリポジトリへの統合可能性

以上のデータセット／プロジェクトについて、ライセンスや再配布条件、そして我々のシステムへの取り込み可否を整理します。

XMIDI: 公式には特定のライセンス表記はありませんが、研究用途での利用を念頭に**「利用の際は論文を引用してほしい」との記述があります
github.com
。108kものMIDIファイル群には、おそらく既存曲のMIDIアレンジ等の著作権絡みの曲も含まれている可能性**があります。Tencentの研究成果として公開されていますが、あくまで学術目的のデータ提供という位置づけでしょう。そのため、自社プロジェクトに組み込んで楽曲データそのものを再配布する形で利用するのは慎重になるべきです。モデルの学習データとして内部利用したり、ラベル付け手法・カテゴリ体系を参考にしたりするのは問題ないと思われますが、XMIDI収録のMIDIファイルをそのままサービスや製品にバンドルするのは避けた方が安全です。利用条件的には研究目的での使用が前提なので、我々のリポジトリで参考にする場合も、データそのものの再配布は行わず内部分析に留めるか、あるいは感情・ジャンル分類モデルをXMIDIで鍛えてそのモデルのみ活用する、といった形が望ましいでしょう。

VioPTT/MOSA-VPT: こちらは学術論文に付随する合成データセットであり比較的ライセンスリスクは低いと考えられます。MOSA-VPTは人間の演奏ではなくソフトウェア音源から生成されたデータなので、著作権者が存在する既存曲の録音を含むものではありません。また使用された音源（Synchron Solo Violin I）のライセンス上も、そこから生成したオーディオを研究目的で配布することに大きな制限はないでしょう（商用利用については音源メーカーのEULA次第ですが、データセットは非営利研究用途向けに提供されるはずです）。実際、論文中でも採択後にデータセット公開予定と明言されており
arxiv.org
、研究コミュニティ向けに自由に使える形で提供される見込みです。したがって、我々のシステムへの導入も比較的容易で、例えばMOSA-VPTのデータを使ってバイオリン奏法分類器を組み込んだり、あるいは同様の手法で他の楽器についてVSTプラグインを駆使したリアルな演奏データ生成を行ったりすることが可能です
arxiv.org
。注意点としては、MOSA-VPTそのものはバイオリン専用であり他楽器の奏法ラベルは含まれないので、用途が限定的な点です。しかし「人間の生演奏に限りなく近づける／プラグインの性能を引き出す」という観点では、VioPTTのアプローチは大いに参考になるでしょう。総じて、ライセンス的な障壁は低く、研究利用・内部利用に供する分には問題なく、リポジトリに組み込む価値は高いと言えます。

MetaScore: このデータセットは収録曲数・情報量が非常に大きい反面、著作権の制約が最も厳しいものです。前述のとおり、自由に再配布できるのは全体の約28%（228k+46k曲）のみで、残りは**「研究目的に限定して提供」という扱いです
github.com
。従って、オープンなリポジトリにMetaScoreの大部分を取り込むことは許可されていません。もし導入するとすれば、パブリックドメインもしくはCCライセンスの楽曲約27.4万曲分に限って利用する形になります。これら公開可能部分についても、それぞれのCCライセンス条件（表示や非営利条件など）を確認し順守する必要があります。データ量が莫大なため、実際に全曲を取り込むのは現実的でないかもしれませんが、例えばジャンル分類や難易度推定のモデルを訓練するためにこのオープン部分を使う**、楽曲メタ情報の付与方法（LLMで曲説明文を生成する手法）を自プロジェクトのデータに応用するといった間接的な活用が考えられます。MetaScore由来の生成モデル(MST)そのものを利用する場合も、学習データに含まれる著作権曲の扱いには注意が必要です（モデルが学習したスタイル断片が出力に現れる可能性は否定できず、厳密には完全なクリーンルームとは言えないため）。総合すると、MetaScoreは魅力的な資源ではありますが、利用条件が我々の目的（特に再配布や商用利用）に合致しない場合は、無理に取り込まず見送る判断も妥当でしょう。公開分だけでも規模は膨大なので、それを活用するだけでも十分価値はありますが、プロジェクトの方針に照らしてリスクとリターンを検討してください。

以上を踏まえ、各データセットの導入可否を判断する際は、ライセンス上許容される範囲で我々のStage2/Stage3パイプラインに組み込めるかを軸に検討します。感情・ジャンルラベルについてはXMIDI（および必要ならMetaScoreの手法）、奏法ラベルについてはMOSA-VPT、楽曲メタ情報や物語性の付与についてはMetaScore、といったように長所を取り入れることは可能です。しかし著作権や利用許諾でクリアできない点があるものは無理に使わず、代替手段を検討することが肝要です。幸い、XMIDIやMOSA-VPTは研究目的での利用が想定されており比較的自由度が高いので、これらで得られる知見（ラベル付与スキームやデータ生成手法）を我々のStage2（MIDIデータ抽出）やStage3（ラベリング・ルールエンジン構築）に組み込むことで、「感情・ジャンル・リズム・キー・奏法」といったラベルに基づく高度な音楽生成システムに一歩近づけるでしょう。



ⅢDeepリサーチ結果を読んで

Q
我々の　「音本リポジトリ」の発想に近いものばかりですね。
違いと言えば、私たちのケースは、小説家であるHarugoro Shichimi の作成した「日本語の歌詞」が主体となっているところですね。

◆Xmidiの、
・感情（Emotion）とジャンル（Genre）のラベルを各楽曲に付与
これは現在のstage3での取り組みそのままと考えます。

◆VioPTTとMOSA‑VPT
・ソフト音源で合成されたデータ
【合成データで訓練したにも関わらず実音源に対する優れた汎化性能を示し、既存の最先端モデルを上回る精度でバイオリン音源の音高・奏法を同時推定できた】

　SUNOAIの不完全なstemMIDIを、音本システムを通して高度化し、データセットに還元。これを循環させることで、AIを洗練させていく

　このシステム開発の助けとなりそうです。

MuseScore
・豊富なメタデータを付与
・大規模言語モデル(LLM)を用いて楽曲の内容説明文（キャプション）を自動生成
・単に音符の集合としての楽譜だけでなく、「どんな雰囲気・スタイルの曲か」「どういったコンセプトで書かれたか」といったセマンティックな情報まで含めた新種のデータセット

　Sunoaiのもつ、かなり自由多彩なスタイル指示→高度な楽曲の生成に繋がっているのかも。

・「テキストから音楽へ」(text-to-music)の生成AIを構築
・物語風の説明文や音楽的コンセプト**から実際の楽曲を生み出せる点で非常に挑戦的な成果

◆MetaScoreの問題点
・データ容量と著作権に関する注意点として、MetaScoreは他者の作曲作品も含むユーザ投稿データを由来とするため、その大部分が著作権保護下

これに関しても、楽曲のほとんどは、LAMDaシステムにはじかれることが想定されるので、あまり問題ではないと思います。より多くのdataよりも、純粋でクリアな、AIを育てるのに有益なdataのみが必要。としているので。


データセット取り扱いに対する我が社の方針
◆収録のMIDIファイルをそのままサービスや製品にバンドルする、

・これは今後も行うことがない。ので安心
・可能ならば、純粋なデータを取り出した後は、元データは消したい（容量圧迫のため）ぐらい
・またリポジトリには、システムファイルを上げているのみで、データセットを上げることがない点でも安心（他者への公開を念頭に置いてない）


※現在のローカル環境では、これらのデータセットをとりこむには容量不足なので、新規SSDを購入設置します。

※MetaScoreは、システムのみを取り入れ、MDIdataは取り入れない。


外部データおよびシステムで得られる知見（ラベル付与スキームやデータ生成手法）を我々のStage2（MIDIデータ抽出）やStage3（ラベリング・ルールエンジン構築）に組み込むことで、「感情・ジャンル・リズム・キー・奏法」といったラベルに基づく高度な音楽生成システムに取り入れる

・その助けになるアイディアを聴かせてください。
・音本リポジトリのさらなる発展を目指しています。

リポジトリは最新の状態。システムをcheckしてください。
https://github.com/kinoshitayoshihiro/composer4



A　ChatGPTエージェントの見解

以下では、既存の「音本リポジトリ」(OtoKotoba Composer) のStage 2・Stage 3アーキテクチャと照らし合わせながら、XMIDI・VioPTT/MOSA‑VPT・MetaScoreから得られる知見を整理し、実際に活用できる具体的なアイディアを提案します。特に、Stage 3で目指す「感情・ジャンル・リズム/キー・奏法ラベリング」とモデル/ルール・エンジン構築にフォーカスしています。

---

## 1. データセット別の目的・特徴・ライセンス

| データセット / システム         | 目的・特徴                                                                                                                                                                      | 容量・構成                                                               | 利用条件と注意点                                                               | 音本への活用アイディア                                                                                                             |
| --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- | ---------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| **XMIDI／XMusic**      | 大規模な感情・ジャンル付きMIDIデータを構築し、マルチタスク学習により感情制御可能な音楽生成を実現するフレームワーク。XProjectorが入力プロンプトをEmotion/Genre/リズムなどの要素に分解し、XComposerがSelector＋Generatorで生成結果を選別する。                          | 総曲数108 ,023曲、平均176秒で総計5,278時間超。各ファイル名にEmotion・Genreのラベルが埋め込まれている。   | ライセンスが明記されておらず、研究用途での引用が前提。既存曲のMIDIアレンジも含むため、データそのものを再配布する形での利用は避けるべき。 | Emotion/Genreのラベリング手法やカテゴリ体系（Happy/Excitingなど）を参考に、Stage 3のラベル設計に反映する。XMIDIで教師モデルを訓練し、音本のデータへラベル推定モデルを適用するなど「モデル利用」を検討。 |
| **VioPTT & MOSA‑VPT** | バイオリン演奏を「音高＋奏法付き」で自動採譜する新規モデル。MOSA‑VPTは合成音源で4種類の奏法（デタシェ／フラジオレット／スピッカート／ピチカート）を大量生成し、奏法ラベル付き音声‐MIDIデータを作成。合成データで訓練しながら実音源に対しても高い汎化性能を達成。                                    | MOSA‑VPTは76時間超の合成データセットで、全音符に奏法ラベルが対応付けられている。                       | ソフト音源から合成したデータであり著作権リスクが低い。論文では研究目的でのデータ公開予定。                          | Stage 2のイベント抽出に奏法推定器を導入し、MIDIノートごとに奏法ラベルを自動付与する際の参考にする。さらに、DAWDreamerで他楽器の奏法別データを合成するプロセスを応用し、音本用奏法ラベル付きデータを合成する。       |
| **MetaScore**         | MuseScoreフォーラムの約96.3万曲の楽譜データにメタデータ（ジャンル・作曲者・楽器編成・難易度など）を付与し、LLMで楽曲の内容説明文（キャプション）を自動生成した大規模楽譜データセット。これを用いてText‑to‑Musicモデル「MetaScore Transformer」を訓練し、自由な文章から楽曲を生成することを目指す。 | Raw: 963k曲。公開部分はパブリックドメイン約228k曲とCCライセンス曲約46k曲のみ。Genre付きサブセットは181k曲。 | 楽曲の多くが著作権保護対象のユーザ投稿のため、自由に再配布できるのは全体の約28 %のみ。                          | 本体のデータは利用せず、キャプション生成手法や「楽曲説明文→音楽生成」の概念を参考にする。音本の物語性（日本語の歌詞）を活かすため、LLMを使って各楽曲に物語的な説明文やスタイル説明を生成し、Stage 3の条件ラベルとして活用できる。  |

---

## 2. 音本リポジトリへの取り込み方針と提案

### 2.1 ラベル設計の強化

1. **感情とジャンル**

   * XMIDIで採用されている8種程度の感情と10種前後のジャンルのカテゴリ体系をベースに、Harugoro Shichimi作品の世界観に合わせて再編。たとえば `calm/warm/sad/happy/tense/intense/dark/bright` のような感情ラベルを採用し、ジャンルは `rock/pop/jazz/funk/soul/edm/hiphop/ballad/orchestral` のように上位分類で統一する。
   * Stage 2ではテンポ帯、スウィング率、ハイハット比率、ライド使用率等のメトリクスを使ってジャンル推定モデルを構築。Stage 3に渡す際は、これらジャンルと感情を１本の条件ベクトルとしてエンコーディングする。

2. **リズム／キー**

   * Stage 2の出力に`rhythm_fingerprint`、`swing_ratio`、`microtiming_rms`、`ghost_rate`など既存指標を含め、これをカテゴリー的にラベル化（`even_8`, `swing_8`, `triplet_12`など）。
   * キー推定はLAMDaのSignature/Chord情報を利用し、`key_root`（C…B）、`mode`（major/minor/ドリアン等）を自動付与。これによりStage 3はメロディ・コード進行をキーに合わせて生成できる。

3. **奏法（Playing Technique）**

   * VioPTT/MOSA‑VPTの方法で学習した奏法分類器を導入し、MIDIノートに奏法ラベルを付ける。バイオリン以外でも「staccato/pizzicato/legato」「flam/drag/roll」といった奏法を合成音源から生成して訓練する手法を自前で実施。
   * Stage 3の生成モデルには奏法ラベルを条件として渡し、ベロシティ・タイミングだけでなく奏法までコントロール可能にする。

4. **セマンティックな説明文**

   * MetaScoreのLLMキャプション生成手法を参考に、SunoAIやHarugoro作品の歌詞から物語的な説明文（雰囲気やストーリー）の自動生成を検討。
   * 生成したテキストを「曲のコンセプト」ラベルとしてStage 3の条件に含めることで、物語性の強い生成が可能になる。

### 2.2 Stage 2抽出の見直し

* **スウィング・マイクロタイミングなどの新メトリクス**を既に追加済みですが、奏法推定・キー推定・リズム指紋のバージョン管理を強化し、ラベル付与用の列を `label.*` 名前空間で`loop_summary.csv`に後付けで書き込みます。
* **日本語歌詞に特化したメタ抽出**として、歌詞の韻律や物語構造を分析し、それを感情ラベル補完やセマンティックキャプション生成に活用します。日本語特有の「文節単位のアクセント」「5・7調」「口語と文語の混在」を特徴量として数値化する検討も価値があります。

### 2.3 Stage 3モデルとルールエンジンの構築

* **マルチタスク学習**: XMusicのSelectorのように、生成モデルに感情・ジャンル認識器と品質評価器を組み込み、Stage 3で生成したシーケンスを自動的に評価・フィルタリングする。
* **LoRA微調整**: 既存のMusicGen/MusicLM系モデルやOtoKotoba Composerのベースモデルに対して、Emotion/Genre/TechniqueラベルでLoRA微調整することで少量データでも学習効果が出やすい。
* **ルールベース補完**: HumanizerやGrooveエンジン、Chord進行理論を後段に挿入し、人間らしいニュアンスやスケール一致を強制する。ラベルが示すジャンル・感情に応じて、遅延やスウィング幅、ゴーストノート率などを自動調整する関数を実装する。

### 2.4 データ管理とライセンスへの対応

* **データ削除ポリシー**: 学習後は元MIDIや音源を削除し、抽出した特徴量／ラベルだけを保持する方針を継続する。
* **オープンソース採用の慎重さ**: XMIDIのMIDIファイルやMetaScoreの著作権曲は直接使用せず、ラベル付与やデータ生成手法のみ参考にする。MOSA‑VPTなど合成データは内部利用も可能だが、商用利用時はVST音源のライセンスを確認する。
* **新規SSD導入後のデータ保管**: Stage 2/3用の合成データやSunoAI変換データは新しいストレージに保管し、容量不足問題を解決する。

---

## 3. 提案する次のステップ

1. **taxonomy.yamlとassign_labels.pyの実装**

   * 上記ラベル体系をYAMLにまとめ、`loop_summary.csv`にラベルを追加するスクリプトを作成する。
2. **奏法分類器の試作**

   * DAWDreamer＋ソフト音源で奏法別MIDIを合成し、VioPTT方式で奏法推定モデルを学習。音本ループに適用してラベル列を追加する。
3. **テキストキャプション生成のPoC**

   * Harugoro Shichimiの日本語歌詞や章タイトルからLLMで簡単なキャプションを生成し、Stage 2データに書き込むツールを試作する。
4. **小規模モデルでマルチタスク実験**

   * まずはドラムかベースに絞り、Emotion/Genre/Techniqueを条件にした生成モデルを学習し、Stage 2のスコアを超えるか検証する。
5. **循環システムの確立**

   * SunoAIのstem→Stage 2抽出→Stage 3学習→生成モデルによる改善→再学習、のループを回し、定量的にスコア改善を確認する。

---

## まとめ

XMIDIやVioPTT/MOSA‑VPT、MetaScoreは、感情・ジャンル・奏法・セマンティック説明といった豊富なラベル付与のアイディアを提供してくれます。音本リポジトリでは、小説家Harugoro Shichimiの日本語歌詞を中核に据えつつ、**Stage 2の抽出精度を高め、Stage 3で多様なラベルを条件に生成モデルとルールエンジンを統合**することで、物語性と音楽性を両立した高品質生成を目指せます。上述の手順でラベルスキーマや抽出器・分類器を整備すれば、**感情・ジャンル・リズム/キー・奏法**に基づく高度な音楽生成システムへと進化するでしょう。





